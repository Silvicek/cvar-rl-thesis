%************************************************
\chapter{Preliminaries}\label{ch:prelim}
%************************************************

The goal of this chapter is to provide a formal background on the material together with a unified notation (which differs quite a lot from publication to publication). The described topics include reinforcement learning, risk-averse measures and more and are by no means covered in detail. The interested reader is welcome to explore the books and publications referenced throughout this chapter and in section \ref{sec:prelim:literature}. An informed reader may choose to skip to section \ref{sec:prelim:problem} where we formalize the problems tackled in this thesis.

\section{Reinforcement Learning}\label{sec:prelim:rl}


\subsection{Markov Decision Processes}
An MDP is a 5-tuple $\mathcal{M} = (\mathcal{X}, \mathcal{A}, R, P, \gamma)$, where $\mathcal{X}$ and $\mathcal{A}$ are the finite state and action spaces; $R(x, a) \in [R_{\min}, R_{\max}]$ is a random variable representing the reward generated by being in state $x$ and selecting action $a$; $P(\cdot|x, a)$ is the transition probability distribution; $\gamma \in [0, 1)$ is a discount factor. We also assume we are given a starting state $x_0$.


A stationary (or markovian) policy is a mapping from states to actions $\pi:\mathcal{X} \to \mathcal{A}$.

We define the return $Z^\pi(x_t)$ as a random variable representing the discounted reward along a trajectory generated by the MDP by following the policy $\pi$, starting at state $x_t$

\begin{equation}
Z^\pi(x_{t})=\sum_{t=0}^\infty \gamma^tR(x_t,\pi(x_t))
\end{equation}

We will sometimes omit the $\pi$ superscript when the policy is clear from context.

\subsection{Bellman equation}
The Bellman equation is a recursive equation that defines the action-value function Q:
\begin{equation}
Q^\pi(x, a) = \mathbb{E}\left[ Z^\pi(x, a) \right] = \mathbb{E}\left[ Z^\pi(x, a) \right]
\end{equation}
where the next state $X'$ is sampled according to the MDP's transition probabilities.

\section{Distributional Reinforcement Learning}


%*****************************************
%*****************************************
%*****************************************


\section{Risk-Sensitivity}\label{sec:prelim:risk}

\subsection{general}

\subsection{var}

\subsection{cvar}

Let $Z$ be a bounded-mean random variable, i.e. $\mathbb{E}[|Z|] < \infty$, with cummulative distribution function (c.d.f.) $F(z) = \mathbb{P}(Z \le z)$.
In this paper we interpret $Z$ as a reward\footnote{This is in accordance with reinforcement learning literature and opposed to risk-related literature.}. The value-at-risk (VaR) at confidence level $\alpha \in (0,1)$ is the $\alpha$ quantile of $Z$, i.e. 

\begin{equation}
\text{VaR}_\alpha(Z)=F^{-1}(\alpha)=\max\left\lbrace z | F(z) \le \alpha \right\rbrace
\end{equation}

We will use the notation $\text{VaR}_\alpha(Z)$, $F^{-1}(\alpha)$ interchangebly, often explictly denoting the random variable of inverse c.d.f. as $F^{-1}_Z(\alpha)$.

\subsection{Conditional Value-at-Risk}
The conditional value-at-risk (CVaR) at confidence level $\alpha \in (0,1)$ is defined as:

\begin{equation}
\text{CVaR}_\alpha(Z) = \dfrac{1}{\alpha}\int_0^\alpha F^{-1}_Z(\beta) \text{d}\beta = \dfrac{1}{\alpha}\int_0^\alpha \text{VaR}_\beta(Z) \text{d}\beta
\end{equation}

We will also use the following equivalent formulation from \cite{rockafellar2000optimization}:

\begin{equation}\label{eq:cvardef}
\text{CVaR}_\alpha(Z)=
\max_s\left\lbrace \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z-s)^-\right] + s  \right\rbrace 
\end{equation}

where $(x)^- = \min(x, 0)$ represents the negative part of $x$.


\subsection{Time-consistency}
An important property of CVaR MDPs is that of time-consistency. ***Choose one definition, describe*** The notion of time consistency varies from author to author; in Shapiro [XXX] it is shown ***.

*** Policy gradient literature ignores the time consistency-issue, leading to locally optimal policies *** show that they can be worse than EXP ***.


%*****************************************
%*****************************************
%*****************************************

\section{Problem Formulation}\label{sec:prelim:problem}


The risk-sensitive problem we wish to adress for a given confidence level $\alpha$ is as follows:
\begin{equation}
\max_\pi \text{CVaR}_\alpha(Z^\pi(x_0))
\end{equation}



%*****************************************
%*****************************************
%*****************************************

\section{Literature Survey}\label{sec:prelim:literature}


