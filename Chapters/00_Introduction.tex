%************************************************
\chapter{Introduction}\label{ch:intro}
%************************************************

A staple of an intelligent agent is the ability to reason and act over time in an environment, while working towards a desirable goal. This is the setting explored in reinforcement learning (RL), a branch of machine learning that focuses on dynamic decision making in unknown environments.

Recent advances in artificial intelligence (AI) are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapon systems. As the industry steps away from specialized AI systems towards more general solutions, the demand for safe approaches to artificial intelligence increases.

In this thesis, we tackle one aspect of safe reinforcement learning, robustness, by considering the risk involved in acting in a non-deterministic, noisy environment.

\section{Motivation}\label{sec:intro:motivation}

Lately, there has been a surge of successes in machine learning research and applications, ranging from visual object detection \cite{krizhevsky2012imagenet} to machine translation \citep{bahdanau2014neural}. Reinforcement learning has also been a part of this success, with excelent results regarding human-level control in computer games \cite{mnih2015human} or beating the best human players in the game of Go \citep{silver2017mastering}. While these successes are certainly respectable and of great importance, reinforcement learning still has a long way to go before being applied on critical real-world decision-making tasks. This is partially caused by concerns of safety, as mistakes can be costly in the real world.

One of the problems encountered when training a reinforcement learning agent is sample efficiency, or the large amount of training time needed for the agent to successfully learn new and correct behaviors. The solution used by many \citep{many} is to train the agent in simulation - it is indeed faster (as the simulation can run in parallel or faster than real-time), safer (we do not face any real danger in simulations) and cheaper than to train the agent in the real world.
This approach then raises the question if an agent trained in simulation would perform well outside of the simulation.

Robustness, or distributional shift, is one of the identified issues of AI safety \citep{leike2017ai, amodei2016concrete, leike2017ai} directly tied to the discrepancies between the environment the agent trains on and is tested on. \citet{chow2015risk} have shown that risk, a measure of uncertainty of the potential loss/reward, can be seen as equal to robustness, taking into account the differences during train- and test-time. This point is discussed in more detail in chapter \ref{ch:prelim}.

While the term risk is a general one, we will focus on a concrete notion of risk - a particular risk metric called Conditional Value-at-Risk (CVaR). 
Due to it's favorable computational properties, CVaR has been recognized as the industry standard for measuring risk in finance, as in 2014 the Basel Committee on Banking Supervision changed its guidelines for banks to replace VaR (a previously used metric) with CVaR for assessing market risk \citep{basel2013fundamental}. The metric also satisfies the recently proposed axioms of risk in robotics \citep{majumdar2017should}.

Another motivational point, aside from robustness, might be one of general decision-making. Commonly encountered in finance, decision makers face the problem of maximizing profits while keeping the risks to a minimum. The solutions to problems encountered in this thesis can therefore be seen as ones of general time-dependent risk-averse decision making.

The aim of this thesis is to consider reinforcement learning agents that maximize Conditional Value-at-Risk instead of the usual expected value, hereby learning a robust, risk-averse policy. The word \textit{distributional} in the title emphasizes that our approach takes inspirations from, or directly uses, the recent advances in distributional reinforcement learning \citep{bellemare2017distributional, dabney2017distributional}.


%************************************************

\section{Thesis Outline}

We begin the thesis by formaly defining the 

\todo{~half a page}

%************************************************


\section{Original Contributions}

\todo{page+}
To the best of our knowledge, this is the first sampling

\begin{enumerate}
\item \textbf{Fast CVaR Value Iteration:} In \chref{vi} we leverage a connection between the $\acvara$ function and the quantile function of the underlying distributions, and propose a procedure for computing fast CVaR Value Iteration updates. The original approach requires computating a linear program each iteration. In contrast, the proposed procedure is linear-time and therefore allows scaling CVaR Value Iteration on much larger environments.

We prove the validity of the improved procedure by showing it is equivalent to solving the original convex problem. We formally prove this for strictly increasing distributions and empirically verify the procedure for general distributions.

\item \textbf{CVaR Q-learning:} The CVaR Value Iteration improvement opens a door for a sampling version of the algorithm which we explore in \chref{qlearning}. Using methods of recursive VaR-CVaR estimation, we formulate a Temporal Difference update equation that finds the optimal value function in expectation and formulate a new algorithm called CVaR Q-learning. 

We then experimentally verify the validity of our approach by showing that the algorithm learns a risk-sensitive policies on different confidence levels.

\item \textbf{CVaR policy improvement:} 

\item \textbf{Deep CVaR Q-learning:} The methods explored in this thesis should ultimately be usable on large state-spaces where exact Q-learning becomes intractable. In \chref{dqn} we extend CVaR Q-learning to an 
\end{enumerate}

% leading to the first globally optimal linear-time algorithm for CVaR MDPs.

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
