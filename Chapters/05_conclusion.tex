%************************************************
\chapter{Conclusion}\label{ch:conclusion}
%************************************************

In this thesis, we tackled the problem of dynamic risk-averse reinforcement learning. Specificaly we focused on optimizing the Conditional Value-at-Risk objective in a time-dependent Markov Decision Process, using methods of Reinforcement Learning.

The work mainly builds on the CVaR Value Iteration algorithm \citep{chow2015risk}, a dynamic programming method for solving CVaR MDPs. 

Our first original contribution is the proposal of a different computation procedure for CVaR value iteration. The novel procedure reduces the computation time from polynomial to linear. More specifically, our approach does not require solving a series of Linear Programs and instead finds solutions to the internal optimization problems in linear time by appealing to the underlying distributions of the CVaR function. We formally proved the equivalence of our solution for a subset of probability distributions.

Next we proposed a new sampling algorithm we call CVaR Q-learning, that builds on our previous results. Since the algorithm is sample-based, it does not require perfect knowledge of the environment. 
In addition, we proposed a new policy improvement algorithm for distributional reinforcement learning, proved its correctness and later used it as a heuristic for extracting the optimal policy from CVaR Q-learning. We empirically verified the practicality of the approach and our agent is able to learn multiple risk-sensitive policies all at once.

To show the scalability of the new algorithm, we extended CVaR Q-learning to its approximate variant by formulating the Deep CVaR loss function and used it in a deep learning context. The new Deep CVaR Q-learning algorithm with experience replay is able to learn different risk-sensitive policies from raw pixels.


We believe that the CVaR objective is a practical framework for computing control policies that are robust with respect to both stochasticity and model perturbations. Collectively, our work enhances the current state-of-the-art methods for CVaR MDPs and improves both practicality and scalability of the available approaches. 


%\newpage

\section{Future Work}

Our contributions leave several pathways open for future work. Firstly, our proof of the improved CVaR Value Iteration works only for a subset of probability distributions and it shall be at least theoretically beneficial to prove the same for general distribution. The result may also be necessary for the convergence proof of CVaR Q-learning. Another missing piece required for proving the asymptotic convergence of CVaR Q-learning is the convergence of recursive CVaR estimation. Currently the convergence has been proven only for continuous distributions and more general proof is required to show the CVaR Q-learning convergence.

In \chref{qlearning}, we highlighted a way of extracting the current policy from converged CVaR Q-learning values. While the method is consistent in the limit, for practical purposes it serves only as a heuristic. It remains to be seen if there are better, perhaps exact ways of extracting the optimal policy.

The work of \citet{bauerle2011markov} shares a connection with CVaR Value Iteration and may be of practical use for CVaR MDPs. The relationship between CVaR Value Iteration and Bauerle's work is very similar to the c51 algorithm \citep{bellemare2017distributional} and QR-DQN \citep{dabney2017distributional}. Bauerle's work is also a certain 'transposition' of CVaR Value Iteration and a comparison between the two may be beneficial. Of particular interest is the ease of extracting the optimal policy in a sampling version of the algorithm.

Lastly, our experimental work focused mostly on toy problems that demonstrated the basic functionality of the proposed algorithms. Since we believe our methods are practical beyond these toy settings, we would like to see the techniques applied on relevant problems from the financial sector and in the future on practical robotics, and other risk-sensitive applications.


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
