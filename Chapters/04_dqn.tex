%************************************************
\chapter{Deep CVaR Q-learning}\label{ch:dqn}
%************************************************

A big disadvantage of value iteration and q-learning is the necessity to store a separate value for each state. When the size of the state-space is too large, we are unable to store the action-value representation and the algorithms become intractable. To overcome this issue, it is common to use function approximation together with Q-learning. We follow the work of \citet{mnih2015human} proposed the Deep Q-learning (DQN) algorithm and succesfully trained on multiple different high-dimensional environments, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.

In this chapter, we extend the proposed CVaR Q-learning to it's deep Q-learning variant and show the practicality and scalability of the proposed methods.

\section{Deep Q-learning}
The ultimate goal of artificial inteligence are agents that perform well across wide range of tasks. In the past, research was often focused on narrow AI, which was able to perform well on one particular task, but was useless on others. That changed with the advent of deep learning \cite{neco}, that allowed us to train function approximators with the ability to generalize. Deep learning methods have become popular across all of machine learning, especially for supervised learning and vision.

%At the core of deep learning lie parametrized function approximators, called neural networks, usually of the form $f_\theta(x) = f_n \circ f_{n-1} \circ\hdots\circ  f_1(x)$ where $f_i(x) = g_i(\theta_i x)$ with $g_i$ being usually a nonlinear function and $\theta_i$ is the parameter matrix. Another necessary component of deep learning is the loss function $\mathcal{L}$ to be minimized, the most common loss functions being the mean squared error for regression and cross entropy for classification. The loss function is trained on 
\subsection{Deep Learning}

We include a short glossary of deep learning basics and terminology. There are much better sources to draw from and we refer the reader to \cite{deeplearning multiple} for a comprehensive overview of the field.

Deep learning considers a particular class of parametrized function approximators called \textbf{neural networks}. 
Neural networks are functions of the form $f_\theta(x) = f_n \circ f_{n-1} \circ\hdots\circ  f_1(x)$ where $f_i(x) = g_i(\theta_i x)$ with $g_i$ being usually a nonlinear function called the \textbf{activation function} and $\theta_i$ is a parameter matrix. Every neural network has an input and output and several \textbf{layers}, where a layer represents one matrix multiplication and application of an activation function $f_i(x)$.

The types of neural networks differ, from the simplest with straightforward matrix multiplication called multi layered perceprtons (MLP) to more complicated ones. Arguably the most important layer type that kickstarted the deep learning leverage is called a \textbf{convolutional neural network} or CNN. Convolutional neural networks consists of several (even hundreds in recent vision applications \cite{...}) so called convolutional layers and it's input is usually a 2D image. A convolutional layer consists of rectangular filters that look for increasingly abstract features by applying the same weight transformations over the whole image. The takeway from us is that convolutional layers are able to learn from images, which is a fact we *** in our experiments.

A necessary component of a deep learning algorithm is the loss function $\mathcal{L}$ to be minimized. The most common loss function being the mean squared error
\begin{equation*}
\mathcal{L} = \expval{(f_\theta(x) - \hat{y})^2}
\end{equation*}
with samples $\hat{y}$ representing the outputs drawn from a distribution of interest. 

The loss function is then minimized using Stochastic Gradient Descent algorithm, a stochastic version of the well-known gradient descent (see e.g. \citet{boyd2004convex}) where we perform each gradient step only using a subset of available samples. 
\begin{equation}
\theta_{t+1} = \theta - \beta\nabla_\theta \mathcal{L}
\end{equation}
There exist many improved variants of SGD such as RMS-prop \cite{...} or Adam \cite{...}, that perform significantly better than the vanilla SGD algorithm and are often used in practice (see \cite{survey} for a concise survey on the used algorithms).


\subsection{DQN}

Deep learning has also been succesfully applied to reinforcement learning. Q-learning have been used with function approximators in the past\cite{neco}, however it suffers from instabilities during learning. In fact, it is well-known that Q-learning does not converge when used in conjunction with function approximators \cite{...} and this has been a problem in practice as well. \citet{mnih2015human} were able to stabilize the learning process by introducing two practical techniques. Firstly the model isn't learned online, meaning we see each example once, but instead a replay buffer is used to store transitions $(x, a, r, x')$ and these are later randomly sampled and used repeatedly for the updates. Secondly, a second network $Q'$ is used for computing the target values $r + \gamma Q'(x', a')$ and is only slowly updated towards $Q$. These improvements have helped to stabilize the learning greatly. See \algref{dqn} for the full procedure.

The DQN algorithm has been applied on the Atari Learning Environment \cite{atari}, a set of challenging and diverse tasks, with both inputs and outputs mirroring a human's experience of playing and learning the Atari games, and the same algorithm achieved human and superhuman-level performance on many of these games.

\begin{algorithm}
\caption{Deep Q-learning with experience replay}
\begin{algorithmic}\label{alg:dqn}

    \STATE Initialize replay memory $M$
    \STATE Initialize action-value function $Q$ with random weights $\theta$
    \STATE Initialize target action-value function $Q'$ with weights $\theta'=\theta$

    \FOR{each episode}
    \STATE $x=x_0$
	\WHILE{$x$ is not terminal}
	\STATE Choose $a$ using a policy derived from $Q$ ($\varepsilon$-greedy)
	\STATE Take action $a$, observe $r, x'$
	\STATE Store transition $(x, a, r, x')$ in $M$
	\STATE $x = x'$
	\STATE Sample random transitions $(x_j, a_j, r_j, x_j')$ from $M$
	\STATE Set $T_j=\begin{cases}
      r_j & \text{if episode terminates at step } j+1\\
      r_j + \gamma \max_{a'} Q'(x_j', a') & \text{otherwise}
    \end{cases}$
    \STATE Perform a gradient step on $\bround{T_j - Q(x_j, a_j)}^2$ w.r.t. $\theta$ \todo{Tj->something else}
    \STATE Every $N_\text{target}$ steps set $\theta'=\theta$
	\ENDWHILE
	\ENDFOR
	
\end{algorithmic}
\end{algorithm}

\section{Distributional Reinforcement Learning with Quantile Regression}
Before we transition to CVaR Q-learning, we will mention the Quantile Regression DQN algorithm by \citet{dabney2017distributional}, which shares certain similarities with CVaR Q-learning.

In QR-DQN, which follows up on the theoretical results of \citet{bellemare2017distributional}, the goal is to learn a distribution that minimizes the wasserstein distance from the actual return distribution. The distributions are represented as discrete uniform probability atoms and the ***

...


\section{Deep CVaR Q-learning}
The transition from CVaR Q-learning to Deep CVaR Q-learning (CVaR DQN) follows the same principles as the one from Q-learning to DQN. Recall the TD update rule for CVaR Q-learning:
\begin{align*}
V(x, a, y_i) &= V(x, a, y_i) + \beta \expect_j\bsquare{1 - \dfrac{1}{y_i}\indicator_{(V(x, a, y_i) \ge r+\gamma d_j)}}\\
C(x, a, y_i) &= (1-\beta)C(x, a, y_i) + \beta\expect_j\bsquare{V(x, a, y_i) + \dfrac{1}{y_i}\bround{r+\gamma d_j - V(x, a, y_i)}^-}
\end{align*}
First significant change against DQN or QR-DQN is that we need to represent two separate values - one for $V$, one for $C$. As with DQN, we need to reformulate the updates as arguments minimizing some loss functions.

In DQN, we minimize the mean squared error (MSE) of $\expval{\bround{\cT Q(x, a) - Q(x, a)}^2}$ as the true $Q(x, a)$ is the minimal argument of MSE.

In QR-DQN we instead optimize the quantile loss function $\expect_j\bsquare{(\cT V_j - V_i)(y_j - \indicator_{(V_i \ge \cT V_j)})}$.

In CVaR-DQN we have to fi***


\subsection{Loss functions}
The loss function for $V(x, a, y)$ is similar to QR-DQN loss in that we wish to find quantiles of a particular distribution. The distribution however is constructed differently - in CVaR-DQN we extract the distribution from the $y\cvar_y$ function of the next state.
\begin{equation}
\mathcal{L}_{\var}=\sum_{i=1}^{N} \expect_j\bsquare{(r + \gamma d_j - V_i(x, a))(y_j - \indicator_{(V_i(x, a) \ge r + \gamma d_j)})}
\end{equation}
where $d_j$ are the extracted atoms.

Constructing the CVaR loss function consists of transforming the running mean into MSE, again with the transformed distribution atoms $d_j$
\begin{equation}
\mathcal{L}_{\cvar}=\sum_{i=1}^{N} \expect_j\bsquare{\bround{V_i(x, a) + \frac{1}{y_i} \bround{r + \gamma d_j - V_i(x, a)}^- - C_i(x, a)}^2}
\end{equation}
A perhaps simpler loss function can be constructed by considering the formulation of $\cvar_\alpha$ as the expected value before $\var_\alpha$, leading to the following loss function
\begin{equation}
\mathcal{L}_{\cvar}=\sum_{i=1}^{N} \expect_j\bsquare{\indicator_{(V_i(x, a) \ge r + \gamma d_j)}\bround{r + \gamma d_j - C_i(x, a)}^2}
\end{equation}
This loss can run into trouble if we had multiple atoms with the same values, however we found \todo{try this} that both loss functions performed identically in our experiments.

Putting it all together, we are now able to construct the full CVaR-DQN loss function in \algref{cvardqnloss}.

\begin{algorithm}
\caption{Deep CVaR Loss function}
\begin{algorithmic}\label{alg:cvardqnloss}

    \STATE \textbf{input:} $x, a, x', r$
    \bindent
    \FOR{each $y_i$ }
	\STATE $C(x', y_i) = \max_{a'} C_i(x', a')$
	\ENDFOR
	
	\STATE $\mathbf{d}= \text{extractDistribution}\bround{C(x', \mathbf{y})}$

	
	\STATE $\mathcal{L}_{\var}=\sum_{i=1}^{N} \expect_j\bsquare{(r + \gamma d_j - V_i(x, a))(y_j - \indicator_{(V_i(x, a) \ge r + \gamma d_j)})}$
	\STATE $\mathcal{L}_{\cvar}=\sum_{i=1}^{N} \expect_j\bsquare{\indicator_{(V_i(x, a) \ge r + \gamma d_j)}\bround{r + \gamma d_j - C_i(x, a)}^2}$
	\eindent
	\RETURN $\mathcal{L}_{\var} + \mathcal{L}_{\cvar}$
	
\end{algorithmic}
\end{algorithm}

Combining the loss functions with the full DQN algorithm, we get the full CVaR-DQN with experience replay, see \algref{cvardqn}. Note that we also utilize a target network $C'$ that is used for extraction of the target values, similarly to the original DQN. 
\begin{algorithm}
\caption{Deep CVaR Q-learning with experience replay}
\begin{algorithmic}\label{alg:cvardqn}

    \STATE Initialize replay memory $M$
    \STATE Initialize the VaR function $V$ with random weights $\theta_v$
    \STATE Initialize the CVaR function $C$ with random weights $\theta_c$
    \STATE Initialize target CVaR function $C'$ with weights $\theta_c'=\theta_c$

    \FOR{each episode}
    \STATE $x=x_0$
	\WHILE{$x$ is not terminal}
	\STATE Choose $a$ using a policy derived from $C$ ($\varepsilon$-greedy)
	\STATE Take action $a$, observe $r, x'$
	\STATE Store transition $(x, a, r, x')$ in $M$
	\STATE $x = x'$
	\STATE Sample random transitions $(x_j, a_j, r_j, x_j')$ from $M$
	\STATE Build the loss function $\mathcal{L}_{\var} + \mathcal{L}_{\cvar}$ (\algref{cvardqnloss})
    \STATE Perform a gradient step on $\mathcal{L}_{\var} + \mathcal{L}_{\cvar}$ w.r.t. $\theta_v, \theta_c$
    \STATE Every $N_\text{target}$ steps set $\theta_c'=\theta_c$
	\ENDWHILE
	\ENDFOR
	
\end{algorithmic}
\end{algorithm}

%*****************************************


\section{Experiments}
We present 

\subsection{Atari}
We ran several experiments on the Atari Learning Environment, which is used as a benchmark for many Q-learning algorithms. The CVaR-DQN algorithm was able to learn reasonable policies with similar speed and performance as the original DQN algorithm. Unfortunately, due to the inherent determinism of the ALE, we didn't find significant differences between policies optimizing $\cvar_\alpha$ on different confidence levels.

\subsection{Ice Lake}


\subsection{Backtrading}
would be cool

\section{Summary}
We expect that all the DQN improvements such as experience replay \cite{...}, dueling \cite{...}, parameter noise \cite{...} and others (combining the improvements matters, see \cite{...}) should have a positive effect on the learning performance, although we stick to vanilla DQN in all of our experiments.

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
