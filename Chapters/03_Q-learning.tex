%************************************************
\chapter{Q-learning with CVaR}\label{ch:qlearning}
%************************************************

While value iteration is a useful algorithm, it only works when we have complete knowledge of the environment - including the probability transitions $p(x'|x,a)$. This is often not the case in practice and we have to rely on different methods, often relying on direct interaction with the environment. One such algorithm is the well-known Q-learning which we explore in this chapter.

We first remind the reader of Q-learning basics in \secref{qlearning} and introduce CVaR estimation in \secref{cvarestimation}. These concepts are combined together with CVaR value iteration and in \secref{qcvar} we propose the first ***is it?*** CVaR Q-learning algorithm. We treat the optimal policy separately in \secref{qpolicy}.

The algorithm is then experimentally verified on suitable environments in \secref{qexperiments}.

%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
\subsection{Q-learning}\label{sec:qlearning}

Q-learning (\citet{watkins1992q}) is an important off-policy temporal difference control algorithm, that works by repeatedly updating the $Q$ value estimate according to the sampled rewards and states using a moving exponential average.
\begin{equation}
\begin{split}
&Q_{t+1}(x, a) = (1-\beta)Q_{t}(x, a) + \beta\bsquare{r + \gamma \max_{a'} Q_t(x', a')}\\
&x' \sim p(\cdot|x, a)
\end{split}
\end{equation}
Here $Q$ is an estimate of the optimal action-value function \eqnref{q} and $\beta$ is the learning rate. The order of the visited states is unimportant, as long as all reachable states are updated infinitely often and the learning rate meets a standard condition used in stochastic approximation.
\begin{equation*}
\sum_{t=0}^\infty \beta_t = \infty  \quad \sum_{t=0}^\infty \beta_t^2 < \infty\\
\end{equation*}
See \citet{jaakkola1994convergence} for details.

While the algorithm would converge if we were using e.g. a completely random policy, in practice we often try to speed up the convergence by using a smarter, yet still random policy. See \algref{qlearning} for a full practical procedure.


\begin{algorithm}[h!]
\caption{Q-learning}
\begin{algorithmic}\label{alg:qlearning}
    \STATE Initialize $Q(x, a)$ for all $x \in \cX, a \in \cA$ arbitrarily, and $Q(\text{terminal}, \cdot) = 0$
    
	\FOR{each episode}
		
	\STATE $x = x_0$
	
	\WHILE{$x$ is not terminal}
	\STATE Choose $a$ using a policy derived from $Q$ (e.g. $\varepsilon$-greedy)
	\STATE Take action $a$, observe $r, x'$
	\STATE $Q(x, a) = (1-\beta)Q(x, a) + \beta\bsquare{r + \gamma \max_{a'} Q(x', a')}$
	\STATE $x = x'$	
	\ENDWHILE
	
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{CVaR estimation}\label{sec:cvarestimation}
\begin{align*}
\mu^* &= \text{arg}\min \mathbb{E}(Z-\mu)^2]\\
\sigma^2 &= \min \mathbb{E}[(Z-\mu)^2]\\
VaR &= \text{arg}\min \dfrac{1}{\alpha}\mathbb{E}((Z-s)^-)] + s\\
CVaR &= \min \dfrac{1}{\alpha}\mathbb{E}((Z-s)^-)] + s
\end{align*}

\begin{align*}
\var_\alpha &= \var_\alpha - \beta \bsquare{1-\dfrac{1}{\alpha}\indicator_{(\var \le x)}}\\
\cvar_\alpha &= (1-\beta)\cvar_\alpha + \beta \bsquare{\var_\alpha + \dfrac{1}{\alpha}(x-\var_\alpha)^-}
\end{align*}


%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************

\section{CVaR Q-learning}\label{sec:qcvar}

bardou2009recursive

\begin{algorithm}[h!]
\caption{CVaR Q-learning: big picture}
\begin{algorithmic}
    \STATE Initialize $VaR, CVaR$, e.g. $\var_{y_i}(x, a)=0, \cvar_{y_i}(x, a)=0$ for all $x, a, y_i$
    
	\WHILE{not converged}
	
	\STATE $x = x_0$
	\STATE $s = \var_\alpha(x, \argmax_a \cvar_\alpha(x, a))$
	
	\WHILE{$x$ is not terminal}
	\STATE $a = \varepsilon-\argmax_a CVaRFromS(x, a, s)$
	\STATE $x', r = transition(x, a)$
	\STATE $UPDATE(x, a, x', r)$
	\STATE $s = \dfrac{s-r}{\gamma}$
	\STATE $x = x'$
	\ENDWHILE
	
	\ENDWHILE
\end{algorithmic}
\end{algorithm}

Simplified notation using $V(x, a, y), C(x, a, y)$ for the VaR and CVaR estimates.

\begin{algorithm}[h!]
\caption{UPDATE v3}
\begin{algorithmic}

    \STATE $UPDATE(x, a, x', r)$
    
    (Create the estimate of $\cvar_{y_i}(Z^{\pi^*}(x'))$)
    \FOR{each $y_i$ }
	\STATE $C(x', y_i) = \max_{a'} C(x', a', y_i)$
	\ENDFOR
	
	\STATE generate 'samples' from distribution underlying $C(x', y_i)$
	
	\FOR{each sample $v$, atom $y_i$}
	\STATE $V(x, a, y_i) = V(x, a, y_i) + \beta \bsquare{1 - \dfrac{1}{y_i}\indicator_{(V(x, a, y_i) \ge r+\gamma v)}}$
	\STATE $C(x, a, y_i) = (1-\beta)C(x, a, y_i) + \beta \bsquare{V(x, a, y_i) + \dfrac{1}{y_i}\bround{r+\gamma v - V(x, a, y_i)}^-}$
	\ENDFOR
	
\end{algorithmic}
\end{algorithm}



%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************


\section{Optimal Policy}\label{sec:qpolicy}
\subsection{Policy Improvement}
Recall the primal definition of CVaR (***).
Our goal can then be rewritten as
\begin{equation}\label{eq:goal}
\max_\pi CVaR_\alpha^\pi(Z) = \max_\pi \max_s \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s)^-\right] + s
\end{equation}


It also holds [XXX] that for the maximum, it holds $s^*=\text{VaR}_\alpha$
\begin{equation}
\text{CVaR}_\alpha(Z)=
\max_s\left\lbrace \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z-s)^-\right] + s  \right\rbrace =\dfrac{1}{\alpha}\mathbb{E}
\left[ (Z - \text{VaR}_\alpha(Z))^-\right] + \text{VaR}_\alpha(Z) 
\end{equation}


The main idea of the algorithm \ref{alg:var}, partially explored in \cite{bauerle2011markov}, is as follows: If we knew the value $s^*$ in the solution to equation (***), we could simplify the problem to maximize only

\begin{equation}\label{eq:goal}
\max_\pi CVaR_\alpha(Z) = \max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^*
\end{equation}

Given that we have access to the return distributions, we can improve the policy by simply choosing an action that maximizes CVaR in the first state $a_0 = \text{arg}\max_\pi\text{CVaR}_\alpha(Z^\pi(x_0))$. We can then, as an approximation, set $s= VaR_\alpha(Z(x_0))$ and then only maximize the simpler criterion


\begin{equation}
\max_\pi CVaR_\alpha(Z) = \max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^*
\end{equation}


The algorithm can be seen as coordinate ascent; in the first phase (when we compute CVaR) we maximize w.r.t. $s$ while keeping $\pi$ fixed; in the second phase we fix $s$ and maximize w.r.t. $\pi$.

In the following theorem, we show that this indeed leads to a monotonic improvement over the previous policy.

\begin{theorem}
Let $\pi$ be a fixed policy, $\alpha \in (0, 1]$. By following policy $\pi'$ from algorithm \ref{alg:var}, we will improve $CVaR_\alpha(Z)$ in expectation:

$$CVaR_\alpha(Z^\pi) \le CVaR_\alpha(Z^{\pi'})$$
\end{theorem}

\begin{proof}

Let $s^*$ be a solution to eq \ref{eq:cvardef}. Then by optimizing $\max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z-s^*)^-\right]$, we will monotonely improve the optimization criterion \ref{eq:goal}.
$$
CVaR_\alpha(Z^{\pi}) = \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^* \le \max_{\pi'}\dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^{\pi'}-s^*)^-\right] + s^*
$$

Note the following facts:

\begin{equation}
Z_t = R_t + \gamma Z_{t+1}
\end{equation}

\begin{equation}
\mathbb{E}\left[(Z_t-s)^-\right] = \mathbb{E}\left[(Z_t-s)\mathbb{1}(Z_t\le s)\right]
\end{equation}

\begin{equation}
\mathbb{E}[H(Z)] = \sum_i p_i \mathbb{E}[H(Z_i)]
\end{equation}

The last equation holds if $Z\sim p_i$ is a probability mixture for any function $H$.
%Here $Z_{t+1} \sim P(x_{t+1}\given x_t, a)$ represents the random variable mixture when we do not yet know which transition was sampled.
%We further simplified the notation by assigning indices $i$ to each of the possible $x_{t+1}$.

We can rewrite the criterion as
\begin{equation}
\begin{split}
\mathbb{E}\left[(Z_t-s)^-\right] &= \mathbb{E}\left[(Z_t-s)\mathbb{1}(Z_t\le s)\right] = \mathbb{E}\left[(R_t + \gamma Z_{t+1}-s)\mathbb{1}(Z_{t+1}\le \dfrac{s - R_t}{\gamma})\right]\\
&= \sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[(r_t + \gamma Z(x_{t+1})-s)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\gamma\left(Z(x_{t+1})-\dfrac{s-r_t}{\gamma}\right)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \gamma\sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\left(Z(x_{t+1})-\dfrac{s-r_t}{\gamma}\right)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \gamma\sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\left(Z(x_{t+1}) - \dfrac{s - r_t}{\gamma}\right)^-\right]
\end{split}
\end{equation}


Now let's say we sampled reward $\hat{r}_t$ and state $\hat{x}_{t+1}$, we are still trying to find a policy $\pi^*$ that maximizes 
\begin{equation}\label{eq:sampled x_t+1}
\begin{split}
\pi^* &=\text{arg}\max_\pi \mathbb{E}\left[(Z_t-s)^-\right | \hat{x}_{t+1}, \hat{r}]\\
&= \text{arg}\max_\pi \mathbb{E}\left[\left(Z(\hat{x}_{t+1}) - \dfrac{s - \hat{r}_t}{\gamma}\right)^-\right]
\end{split}
\end{equation}

Where we ignored the unsampled states (since these are not a function of $\hat{x}_{t+1}$) and the multiplicative constant $\gamma$ that will not affect the maximum argument.

At the starting state, we set $s=s^*$. At each following state we select an action according to equation \ref{eq:sampled x_t+1}. By induction we maximize the criterion \ref{eq:var-based criterion} in each step.
\end{proof}

\begin{algorithm}
\caption{VaR-based policy improvement}
\label{alg:var}
\begin{algorithmic}
    \STATE \textbf{input} $\alpha, x_0, \gamma$
    \STATE $a = \text{arg}\max_a CVaR_\alpha(Z(x_0, a))$
    \STATE $s = VaR_\alpha(Z(x_0, a))$
    \STATE $x_t, r_t = \text{envTransition}(x_0, a)$
    \WHILE{$x_t$ is not terminal}
    	\STATE $s = \dfrac{s-r_t}{\gamma}$
    	\STATE $a = \text{arg}\max_a \mathbb{E}\left[(Z(x_t, a)-s)^- \right]$
    	\STATE $x_t, r_t = \text{envTransition}(x_t, a)$
   	\ENDWHILE
\end{algorithmic}
\end{algorithm}

Note that the resulting policy is nonstationary, however we do not need an extended state-space to follow this policy, it is only necessary to remember our previous $s$.

\subsection{Repeated policy improvement}
This policy then could be evaluated again by the distributional Q-learning procedures, however we 

%*****************************************
%*****************************************

\section{Experiments}\label{sec:qexperiments}

\todo{ai safety gridworld}

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
