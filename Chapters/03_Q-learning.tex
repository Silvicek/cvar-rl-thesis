%************************************************
\chapter{Q-learning with CVaR}\label{ch:qlearning}
%************************************************

While value iteration is a useful algorithm, it only works when we have complete knowledge of the environment - including the probability transitions $p(x'|x,a)$. This is often not the case in practice and we have to rely on different methods, often relying on direct interaction with the environment. One such algorithm is the well-known Q-learning which we explore in this chapter.

We first remind the reader of Q-learning basics in \secref{qlearning} and introduce CVaR estimation in \secref{cvarestimation}. These concepts are combined together with CVaR value iteration and in \secref{qcvar} we propose the first ***is it?*** CVaR Q-learning algorithm. We treat the optimal policy separately in \secref{qpolicy}.

The algorithm is then experimentally verified on suitable environments in \secref{qexperiments}.

%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
\subsection{Q-learning}\label{sec:qlearning}

Q-learning (\citet{watkins1992q}) is an important off-policy temporal difference control algorithm, that works by repeatedly updating the $Q$ value estimate according to the sampled rewards and states using a moving exponential average.
\begin{equation}
\begin{split}
&Q_{t+1}(x, a) = (1-\beta)Q_{t}(x, a) + \beta\bsquare{r + \gamma \max_{a'} Q_t(x', a')}\\
&x' \sim p(\cdot|x, a)
\end{split}
\end{equation}
Here $Q$ is an estimate of the optimal action-value function \eqnref{q} and $\beta$ is the learning rate. The order of the visited states is unimportant, as long as all reachable states are updated infinitely often and the learning rate meets a standard condition used in stochastic approximation.
\begin{equation}\label{eqn:beta}
\sum_{t=0}^\infty \beta_t = \infty  \quad \sum_{t=0}^\infty \beta_t^2 < \infty\\
\end{equation}
See \citet{jaakkola1994convergence} for details.

While the algorithm would converge if we were using a completely random policy, in practice we often try to speed up the convergence by using a smarter, yet still random policy. See \algref{qlearning} for a full practical procedure.


\begin{algorithm}
\caption{Q-learning}
\begin{algorithmic}\label{alg:qlearning}
    \STATE Initialize $Q(x, a)$ for all $x \in \cX, a \in \cA$ arbitrarily, and $Q(x_\text{terminal}, \cdot) = 0$
    
	\FOR{each episode}
		
	\STATE $x = x_0$
	
	\WHILE{$x$ is not terminal}
	\STATE Choose $a$ using a policy derived from $Q$ (e.g. $\varepsilon$-greedy)
	\STATE Take action $a$, observe $r, x'$
	\STATE $Q(x, a) = (1-\beta)Q(x, a) + \beta\bsquare{r + \gamma \max_{a'} Q(x', a')}$
	\STATE $x = x'$	
	\ENDWHILE
	
	\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{CVaR estimation}\label{sec:cvarestimation}

Before formulating a CVaR version of Q-learning, we must first talk about simply \emph{estimating} CVaR, as it is not as straightforward as the estimation of expected value.

Let us remind ourselves of the primal definition of CVaR \eqnref{cvarprimal}:
\begin{equation*}
\cvar_\alpha(Z)=
\max_s\left\lbrace \dfrac{1}{\alpha}\expect
\left[ (Z-s)^-\right] + s  \right\rbrace 
\end{equation*}
If we knew the exact $s^*=\var_\alpha$, we could estimate the CVaR as a simple expectation of the $\dfrac{1}{\alpha}(Z-s^*)^-+s^*$ function. As we do not know this value in advance, a common approach is to first approximate $\var_\alpha$ from data, then use this estimate to compute it's $\cvar_\alpha$. This is usually done with a full data vector, requiring the whole data history to be saved in memory \cite{???}.

When dealing with reinforcement learning, we would like to store our current estimate as a scalar instead. This requires finding a recursive expression whose expectation is the CVaR value. Fortunately, similar methods have been thoroughly investigated in the stochastic approximation literature by \citet{robbins1951stochastic}.

The RM theorem has also been applied directly to CVaR estimation by \citet{bardou2009recursive}, who used it to formulate a recursive importance sampling procedure useful for estimating CVaR of long-tailed distributions.

First let us describe the method for a one step estimation, meaning we sample values (or rewards in our case) $r$ from some distribution and our goal is to estimate CVaR at a given confidence level $\alpha$. The procedure requires us to maintain two separate estimates $V$ and $C$, being our VaR and CVaR estimates respectively.
\begin{align}
V_{t+1} &= V_{t} + \beta \bsquare{1-\dfrac{1}{\alpha}\indicator_{(V_t \ge r)}}\label{eqn:varestimate}\\
C_{t+1} &= (1-\beta)C_t + \beta \bsquare{V_t + \dfrac{1}{\alpha}(r-V_t)^-}\label{eqn:cvarestimate}
\end{align}
\todo{$C_0=0$?}

An observant reader may recognize a standard equation for quantile estimation in equation \eqnref{varestimate} (see e.g. \cite{quantile} for more information on quantile estimation/regression) and equation \eqnref{cvarestimate} is also quite intuitive, representing the moving exponential average of the primal CVaR definition \eqnref{cvarprimal}. The estimations are proven to converge, given the usual requirements on the learning rate \eqnref{beta} \citep{bardou2009recursive}.

%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************

\section{CVaR Q-learning}\label{sec:qcvar}

We now extend the previously established CVaR value iteration and combine it with the recursive CVaR estimation techniques to formulate a new algorithm we call CVaR Q-learning \unclear{or off-policy TD? there is no Q}.
\subsection{Temporal Difference update}
We first define two separate values for each state, action and atom $V, C: \cX\times\cA\times\cY\to\real$ where $C(x, a, y)$ represents $\cvar_y(Z(x, a))$ of the distribution, similar to the definition \eqnref{cdef}. $V(x, a, y)$ represents the one-step $\var_y$ estimate, or the estimate of the $y-$quantile of a distribution recovered from $\cvar_y$ by Lemma \ref{thm:varcvarconnection}.

A key to any TD algorithm is it's update rule. The CVaR TD update rule extends the improved VI procedure and we present the full rule in \algref{cvartd}. 

Let us now go through the algorithm and compare the two versions. We first construct a new CVaR (line \ref{alg:cvartd:1}) by greedily selecting actions that yield the highest CVaR for each atom. This step is somewhat skipped in the VI process since we are not working with action-value functions. These values are then transformed to it's underlying distributions (line \ref{alg:cvartd:2}) and used to generate 'samples' $v$ from this distribution. Quotes are used here since we know the distributions and do not have to actually sample - instead we use the quantile values proportionally to their probabilities. Similar technique has been used by \citet{dabney2017distributional}. 

If the atoms aren't uniform, we perform basic importance sampling (line \label{alg:cvartd:3}) to weight each ***sample*** according to it's probability. \todo{is this valid IS?}. Finally, these ***samples*** are used to update the VaR and CVaR estimates.


\begin{algorithm}
\caption{CVaR TD update}
\begin{algorithmic}[1]\label{alg:cvartd}

    \STATE \textbf{input:} $x, a, x', r$
    
    \FOR{each $y_i$ }
	\STATE $C(x', y_i) = \max_{a'} C(x', a', y_i)$ \label{alg:cvartd:1}
	\ENDFOR
	
	\STATE \todo{introduce notation to C, v} \label{alg:cvartd:2}

	\FOR{each $v$, $y_i$}
	\STATE $\beta_i = \beta (y_i-y_{i-1})$  \label{alg:cvartd:3}
	\STATE $V(x, a, y_i) = V(x, a, y_i) + \beta_i \bsquare{1 - \dfrac{1}{y_i}\indicator_{(V(x, a, y_i) \ge r+\gamma v)}}$  \label{alg:cvartd:4}
	\STATE $C(x, a, y_i) = (1-\beta_i)C(x, a, y_i) + \beta_i \bsquare{V(x, a, y_i) + \dfrac{1}{y_i}\bround{r+\gamma v - V(x, a, y_i)}^-}$  \label{alg:cvartd:5}
	\ENDFOR
	
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{CVaR TD learning}
\begin{algorithmic}
    \STATE Initialize $V(x, a, y), C(x, a, y)$ for all $x\in\cX, a\in\cA, y\in\cY$ arbitrarily, and $C(x_\text{terminal}, \cdot, \cdot) = 0$
    
	\FOR{each episode}
		
	\STATE $x = x_0$
	\STATE $a_0 = \argmax_a \cvar_\alpha(x, a)$
	\STATE $s = \var_\alpha(x, a_0)$
	\WHILE{$x$ is not terminal}
	\STATE Choose $a$ using a policy derived from $C, s$ (e.g. $\varepsilon$-greedy)
	\STATE Take action $a$, observe $r, x'$
	\STATE $UPDATE(x, a, x', r)$
	\STATE $s = \dfrac{s-r}{\gamma}$
	\STATE $x = x'$
	\ENDWHILE
	
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Note on convergence}
continuous, out of scope

%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************
%***********************************************************************************************************************************************************


\section{Optimal Policy}\label{sec:qpolicy}
\subsection{Policy Improvement}
Recall the primal definition of CVaR (***).
Our goal can then be rewritten as
\begin{equation}\label{eq:goal}
\max_\pi CVaR_\alpha^\pi(Z) = \max_\pi \max_s \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s)^-\right] + s
\end{equation}


It also holds [XXX] that for the maximum, it holds $s^*=\text{VaR}_\alpha$
\begin{equation}
\text{CVaR}_\alpha(Z)=
\max_s\left\lbrace \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z-s)^-\right] + s  \right\rbrace =\dfrac{1}{\alpha}\mathbb{E}
\left[ (Z - \text{VaR}_\alpha(Z))^-\right] + \text{VaR}_\alpha(Z) 
\end{equation}


The main idea of the algorithm \ref{alg:var}, partially explored in \cite{bauerle2011markov}, is as follows: If we knew the value $s^*$ in the solution to equation (***), we could simplify the problem to maximize only

\begin{equation}\label{eq:goal}
\max_\pi CVaR_\alpha(Z) = \max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^*
\end{equation}

Given that we have access to the return distributions, we can improve the policy by simply choosing an action that maximizes CVaR in the first state $a_0 = \text{arg}\max_\pi\text{CVaR}_\alpha(Z^\pi(x_0))$. We can then, as an approximation, set $s= VaR_\alpha(Z(x_0))$ and then only maximize the simpler criterion


\begin{equation}
\max_\pi CVaR_\alpha(Z) = \max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^*
\end{equation}


The algorithm can be seen as coordinate ascent; in the first phase (when we compute CVaR) we maximize w.r.t. $s$ while keeping $\pi$ fixed; in the second phase we fix $s$ and maximize w.r.t. $\pi$.

In the following theorem, we show that this indeed leads to a monotonic improvement over the previous policy.

\begin{theorem}
Let $\pi$ be a fixed policy, $\alpha \in (0, 1]$. By following policy $\pi'$ from algorithm \ref{alg:var}, we will improve $CVaR_\alpha(Z)$ in expectation:

$$CVaR_\alpha(Z^\pi) \le CVaR_\alpha(Z^{\pi'})$$
\end{theorem}

\begin{proof}

Let $s^*$ be a solution to eq \ref{eq:cvardef}. Then by optimizing $\max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z-s^*)^-\right]$, we will monotonely improve the optimization criterion \ref{eq:goal}.
$$
CVaR_\alpha(Z^{\pi}) = \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^* \le \max_{\pi'}\dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^{\pi'}-s^*)^-\right] + s^*
$$

Note the following facts:

\begin{equation}
Z_t = R_t + \gamma Z_{t+1}
\end{equation}

\begin{equation}
\mathbb{E}\left[(Z_t-s)^-\right] = \mathbb{E}\left[(Z_t-s)\mathbb{1}(Z_t\le s)\right]
\end{equation}

\begin{equation}
\mathbb{E}[H(Z)] = \sum_i p_i \mathbb{E}[H(Z_i)]
\end{equation}

The last equation holds if $Z\sim p_i$ is a probability mixture for any function $H$.
%Here $Z_{t+1} \sim P(x_{t+1}\given x_t, a)$ represents the random variable mixture when we do not yet know which transition was sampled.
%We further simplified the notation by assigning indices $i$ to each of the possible $x_{t+1}$.

We can rewrite the criterion as
\begin{equation}
\begin{split}
\mathbb{E}\left[(Z_t-s)^-\right] &= \mathbb{E}\left[(Z_t-s)\mathbb{1}(Z_t\le s)\right] = \mathbb{E}\left[(R_t + \gamma Z_{t+1}-s)\mathbb{1}(Z_{t+1}\le \dfrac{s - R_t}{\gamma})\right]\\
&= \sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[(r_t + \gamma Z(x_{t+1})-s)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\gamma\left(Z(x_{t+1})-\dfrac{s-r_t}{\gamma}\right)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \gamma\sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\left(Z(x_{t+1})-\dfrac{s-r_t}{\gamma}\right)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \gamma\sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\left(Z(x_{t+1}) - \dfrac{s - r_t}{\gamma}\right)^-\right]
\end{split}
\end{equation}


Now let's say we sampled reward $\hat{r}_t$ and state $\hat{x}_{t+1}$, we are still trying to find a policy $\pi^*$ that maximizes 
\begin{equation}\label{eq:sampled x_t+1}
\begin{split}
\pi^* &=\text{arg}\max_\pi \mathbb{E}\left[(Z_t-s)^-\right | \hat{x}_{t+1}, \hat{r}]\\
&= \text{arg}\max_\pi \mathbb{E}\left[\left(Z(\hat{x}_{t+1}) - \dfrac{s - \hat{r}_t}{\gamma}\right)^-\right]
\end{split}
\end{equation}

Where we ignored the unsampled states (since these are not a function of $\hat{x}_{t+1}$) and the multiplicative constant $\gamma$ that will not affect the maximum argument.

At the starting state, we set $s=s^*$. At each following state we select an action according to equation \ref{eq:sampled x_t+1}. By induction we maximize the criterion \ref{eq:var-based criterion} in each step.
\end{proof}

\begin{algorithm}
\caption{VaR-based policy improvement}
\label{alg:var}
\begin{algorithmic}
    \STATE \textbf{input} $\alpha, x_0, \gamma$
    \STATE $a = \text{arg}\max_a CVaR_\alpha(Z(x_0, a))$
    \STATE $s = VaR_\alpha(Z(x_0, a))$
    \STATE $x_t, r_t = \text{envTransition}(x_0, a)$
    \WHILE{$x_t$ is not terminal}
    	\STATE $s = \dfrac{s-r_t}{\gamma}$
    	\STATE $a = \text{arg}\max_a \mathbb{E}\left[(Z(x_t, a)-s)^- \right]$
    	\STATE $x_t, r_t = \text{envTransition}(x_t, a)$
   	\ENDWHILE
\end{algorithmic}
\end{algorithm}

Note that the resulting policy is nonstationary, however we do not need an extended state-space to follow this policy, it is only necessary to remember our previous $s$.

\subsection{Repeated policy improvement}
This policy then could be evaluated again by the distributional Q-learning procedures, however we 

%*****************************************
%*****************************************

\subsection{$\xi$-computation}

Similarly to \thmref{optimalpolicy}, we need a way to compute the $y_{k+1}=y_{k}\xi^*(x_k)$ to extract the optimal policy. Again, we can skip the LP computation by using the following intuition: $y_{k+1}$ is the portion of $Z(x_{k+1})$ that is present in $\cvar_{y_k}(Z(x_k))$. In the continuous case, it is the probability in $Z(x_{k+1})$ before the $\var_{y_k}(Z(x_k))$ as we show bellow.

\todo{proof for discrete distributions}

\begin{theorem}
Solution to minimization problem \eqnref{cvardecomp} can be computed without optimization by setting
\begin{equation}\label{eqn:xi-claim}
\xi ( x' ) = \dfrac{F_{x'}(F^{-1}_x(\alpha))}{\alpha} 
\end{equation}
\end{theorem}

\begin{proof}
For simplification, we work only with two states: $x'$ the actual sampled state and $\bar{x}'$ representing the other states. The equation then simplifies to

\begin{equation}\label{eqn:cvardecomp2}
\begin{split}
\cvar_\alpha(x, a)&=\min_{\xi} \, p\xi \cvar_{\xi\alpha}(x') + (1-p)\dfrac{1-p\xi}{1-p}\cvar_{\frac{1-p\xi}{1-p}\alpha}(\bar{x}')\\
&=\min_{\xi} \, p\xi \cvar_{\xi\alpha}(x') + (1-p\xi)\cvar_{\frac{1-p\xi}{1-p}\alpha}(\bar{x}')\\
\end{split}
\end{equation}

To find the min we first find the first derivative\footnote{
We used the following identities:
\begin{equation*}
\dfrac{\partial \cvar_{\alpha\xi}}{\partial \xi} = \frac{1}{\xi}\var_{\xi\alpha}-\frac{1}{\xi}\cvar_{\xi\alpha}\quad\quad\quad
\dfrac{\partial \cvar_{\frac{1-p\xi}{1-p}\alpha}}{\partial\xi} = \frac{p}{1-p\xi}\cvar_{\frac{1-p\xi}{1-p}\alpha}	-	\frac{p}{1-p\xi}\var_{\frac{1-p\xi}{1-p}\alpha}
\end{equation*}
} w.r.t. $\xi$

\begin{equation}
\begin{split}
\dfrac{\partial \cvar_\alpha}{\partial \xi} &= p\cvar_{\xi\alpha} + p\xi \dfrac{\partial \cvar_{\alpha\xi}}{\partial \xi} - p\cvar_{\frac{1-p\xi}{1-p}\alpha} + (1 - p\xi)\dfrac{\partial \cvar_{\frac{1-p\xi}{1-p}\alpha}}{\partial\xi}\\
&= p\cvar_{\xi\alpha} + p\xi\left[	\frac{1}{\xi}\var_{\xi\alpha}-\frac{1}{\xi}\cvar_{\xi\alpha}	\right] - p\cvar_{\frac{1-p\xi}{1-p}\alpha} \\&\hspace*{5cm} + (1-p\xi)\left[	\frac{p}{1-p\xi}\cvar_{\frac{1-p\xi}{1-p}\alpha}	-	\frac{p}{1-p\xi}\var_{\frac{1-p\xi}{1-p}\alpha}\right]\\
&= p\cvar_{\xi\alpha} + p\var_{\xi\alpha} - p\cvar_{\xi\alpha} - p\cvar_{\xi\alpha} - p\cvar_{\frac{1-p\xi}{1-p}\alpha} \\&\hspace*{5cm} + \cvar_{\frac{1-p\xi}{1-p}\alpha} - p\var_{\frac{1-p\xi}{1-p}\alpha}\\
&= p\var_{\xi\alpha} - p\var_{\frac{1-p\xi}{1-p}\alpha}
\end{split}
\end{equation}

By setting the derivative to 0 (to find the min), we get
\begin{equation}\label{eqn:varvar}
\var_{\xi\alpha}(x')= \var_{\frac{1-p\xi}{1-p}\alpha}(\bar{x}')
\end{equation}

By inserting claim \eqnref{xi-claim} into \eqnref{varvar} we get the symmetrical claim
\begin{equation}
\dfrac{1-p\xi}{1-p} = \xi(\bar{x}') = \dfrac{F_{\bar{x}'}(F^{-1}_x(\alpha))}{\alpha}
\end{equation}

We rewrite \eqnref{cvardecomp2} as (assuming $\xi$ is the minimum point)

\begin{equation}
\begin{split}
\frac{1}{\alpha} \int_0^\alpha F^{-1}_{x}(t)dt &= p\xi \frac{1}{\xi\alpha} \int_0^{\xi\alpha} F^{-1}_{x'}(t)dt + (1-p\xi)\frac{1-p}{(1-p\xi)\alpha} \int_0^{\frac{1-p\xi}{1-p}\alpha} F^{-1}_{\bar{x}'}(t)\\
&=p \frac{1}{\alpha} \int_0^{\xi\alpha} F^{-1}_{x'}(t)dt + (1-p)\frac{1}{\alpha} \int_0^{\frac{1-p\xi}{1-p}\alpha} F^{-1}_{\bar{x}'}(t)
\end{split}
\end{equation}

This must also hold if we multiply both sides by $\alpha$
\begin{equation}
\int_0^\alpha F^{-1}_{x}(t)dt = p\int_0^{\xi\alpha} F^{-1}_{x'}(t)dt + (1-p)\int_0^{\frac{1-p\xi}{1-p}\alpha} F^{-1}_{\bar{x}'}(t)
\end{equation}
And we take derivations w.r.t. $\alpha$ of both sides
\begin{equation}
F^{-1}_{x}(\alpha) = p\xi F^{-1}_{x'}(\xi\alpha) + (1-p\xi) F^{-1}_{\bar{x}'}(\frac{1-p\xi}{1-p}\alpha)
\end{equation}


By inserting \eqnref{xi-claim} we get
\begin{equation}
\begin{split}
 p\xi F_{x'}^{-1}(\xi\alpha) + (1-p)\xi_2 F_{\bar{x}'}^{-1}\left(\xi_2\alpha\right) &= p\xi F_{x'}^{-1}(F_{x'}(F^{-1}_x(\alpha))) + (1-p\xi) F_{\bar{x}'}^{-1}\left(F_{\bar{x}'}(F^{-1}_x(\alpha))\right)\\
 &= p\xi F_x^{-1}(\alpha) + (1-p\xi)F_x^{-1}(\alpha) = F_x^{-1}(\alpha)
\end{split}
\end{equation}

We've shown that the proposed solution \eqnref{xi-claim} satisfies the minimization constraint \eqnref{varvar} (= is a minimal point) and satisfies the dual decomposition \eqnref{cvardecomp}. (This has been shown only in the differentiated form )

\end{proof}

%*****************************************
%*****************************************
%*****************************************


\section{Experiments}\label{sec:qexperiments}

\todo{ai safety gridworld}

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
