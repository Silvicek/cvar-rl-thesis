%************************************************
\chapter{Value Iteration with CVaR}\label{ch:introduction}
%************************************************

\citet{chow2015risk} have recently proposed an approximate value iteration algorithm for CVaR MDPs. This algorithm requires the computation of a linear program in each step of the value iteration procedure. We utilize a connection between the used $\alpha \cvar_\alpha$ function and the quantile function and propose a linear-time algorithm that substitutes the LP computation, making the CVaR value iteration feasible for much larger MDPs. ***experiments*** The procedure also opens the door for a Q-learning variant of the algorithm.


%*****************************************

\section{CVaR Bellman equation}

%*****************************************

\section{Efficient computation}


\subsection{CVaR Value Iteration}

The results of \cite{chow2015risk} heavily rely on the CVaR decomposition theorem, which we show below

\begin{equation}\label{eq:cvardecomp}
\begin{split}
CVaR_\alpha(x, a)&=\min_{\xi} \sum_{x'} p(x, a, x')\xi(x') CVaR_{\xi\alpha}(x')\\
\text{s.t.}\quad &\sum_{x'}p(x, a, x')\xi(x')=1\\
&0 \le \xi(x')\le \dfrac{1}{\alpha}
\end{split}
\end{equation}

The theorem states that we can compute the $CVaR_\alpha(x)$ as the minimal weighted combination of $CVaR_\alpha(x')$ under a perturbed distribution. \cite{chow2015risk} extended this result to a dynamic programming formulation

\begin{equation}
CVaR_\alpha(x)=\max_a R(x, a) + \gamma CVaR_{\alpha}(x, a)
\end{equation}

and showed that the VI procedure is a contraction and perserves the convexity of $\alpha CVaR_\alpha$. The fixed point of this contraction is then the exact CVaR value.

This algorithm is unfortunately unusable in practice, as the state-space is continuous in $\alpha$. The solution proposed in \cite{chow2015risk} is then to represent the convex $\alpha CVaR_\alpha$ as a piecwise linear function. 

***I definition***

The interpolated Bellman operator is then also a contraction and has a bounded error

\begin{equation}
\begin{split}
CVaR_\alpha(x, a)&=\min_{\xi} \sum_{x'} p(x, a, x')\dfrac{I_{x'}(\alpha \xi(x'))}{\alpha}\\
\text{s.t.}\quad &\sum_{x'}p(x, a, x')\xi(x')=1\\
&0 \le \xi(x')\le \dfrac{1}{\alpha}
\end{split}
\end{equation}

\subsection{Quantile representation}

We use the following two facts: firstly, any disrete distribution function has a piecewise linear $\alpha CVaR_\alpha$ function \cite{rockafellar2000optimization}; secondly the $\alpha CVaR_\alpha$ and the quantile function can be computed from each other by utilizing the relation

\begin{equation}
\dfrac{\partial}{\partial \alpha} \alpha CVaR_\alpha(Z) = \dfrac{\partial}{\partial \alpha} \int_0^\alpha VaR_\beta(Z) d\beta = VaR_\alpha(Z)
\end{equation}

***integration constant***

We propose the following improvement: instead of using linear programming for the CVaR computation, we instead use the distributions represented by the $\alpha CVaR_\alpha$ function. 

The computation of CVaR of a discrete probability mixture is a linear-time process as we show bellow. The general steps of the computation are as follows

\begin{enumerate}
\item transform $\alpha CVaR_\alpha$ of each possible state transition to a discrete probability distribution function
\item combine these to to a distribution representing the full state-action distribution
\item compute $\alpha CVaR_\alpha$ for all atoms
\end{enumerate}




%\begin{algorithm}
%\caption{CVaR computation}
%
%\begin{multicols}{2}
%
%\begin{algorithmic}
%    \STATE \textbf{input} $\alpha, x_t, \pi_\text{old}, \gamma$
%    \WHILE{$x_t$ is not terminal}
%    	\STATE $a = \text{arg}\max_a CVaR_\alpha(Z(x_t, a))$
%		\STATE $s = VaR_\alpha(Z(x_t, a))$
%		\columnbreak
%
%    	\STATE $x_t, r_t = \text{envTransition}(x_t, a)$
%    	\STATE $\alpha = F_{Z(x_t, \pi_\text{old}(x_t))}\left(\dfrac{s - r}{\gamma}\right) $ \textcolor{gray}{\# $VaR_\alpha(Z(x_t, \pi_\text{old}(x_t))) == \dfrac{s - r}{\gamma}$}
%   	\ENDWHILE
%\end{algorithmic}
%\end{multicols}
%\end{algorithm}
%
%\begin{minipage}[t]{0.5\linewidth}
%  \vspace{0pt}  
%  \begin{algorithm}[H]
%    \caption{Algo 1}
%    line 1\;
%    line 2\;
%  \end{algorithm}
%\end{minipage}%
%\begin{minipage}[t]{5cm}
%  \vspace{0pt}
%  \begin{algorithm}[H]
%    \caption{Algo 1}
%    line 1\;
%  \end{algorithm}
%\end{minipage}


\subsection{Proof}

%*****************************************

\section{VaR-based Policy Improvement}
\subsection{Policy Improvement}
Recall the primal definition of CVaR (***).
Our goal can then be rewritten as
\begin{equation}\label{eq:goal}
\max_\pi CVaR_\alpha^\pi(Z) = \max_\pi \max_s \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s)^-\right] + s
\end{equation}


It also holds [XXX] that for the maximum, it holds $s^*=\text{VaR}_\alpha$
\begin{equation}
\text{CVaR}_\alpha(Z)=
\max_s\left\lbrace \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z-s)^-\right] + s  \right\rbrace =\dfrac{1}{\alpha}\mathbb{E}
\left[ (Z - \text{VaR}_\alpha(Z))^-\right] + \text{VaR}_\alpha(Z) 
\end{equation}


The main idea of the algorithm \ref{alg:var}, partially explored in \cite{bauerle2011markov}, is as follows: If we knew the value $s^*$ in the solution to equation (***), we could simplify the problem to maximize only

\begin{equation}\label{eq:goal}
\max_\pi CVaR_\alpha(Z) = \max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^*
\end{equation}

Given that we have access to the return distributions, we can improve the policy by simply choosing an action that maximizes CVaR in the first state $a_0 = \text{arg}\max_\pi\text{CVaR}_\alpha(Z^\pi(x_0))$. We can then, as an approximation, set $s= VaR_\alpha(Z(x_0))$ and then only maximize the simpler criterion


\begin{equation}
\max_\pi CVaR_\alpha(Z) = \max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^*
\end{equation}


The algorithm can be seen as coordinate ascent; in the first phase (when we compute CVaR) we maximize w.r.t. $s$ while keeping $\pi$ fixed; in the second phase we fix $s$ and maximize w.r.t. $\pi$.

In the following theorem, we show that this indeed leads to a monotonic improvement over the previous policy.

\begin{theorem}
Let $\pi$ be a fixed policy, $\alpha \in (0, 1]$. By following policy $\pi'$ from algorithm \ref{alg:var}, we will improve $CVaR_\alpha(Z)$ in expectation:

$$CVaR_\alpha(Z^\pi) \le CVaR_\alpha(Z^{\pi'})$$

\end{theorem}

\begin{proof}

Let $s^*$ be a solution to eq \ref{eq:cvardef}. Then by optimizing $\max_\pi \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z-s^*)^-\right]$, we will monotonely improve the optimization criterion \ref{eq:goal}.
$$
CVaR_\alpha(Z^{\pi}) = \dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^\pi-s^*)^-\right] + s^* \le \max_{\pi'}\dfrac{1}{\alpha}\mathbb{E}
\left[ (Z^{\pi'}-s^*)^-\right] + s^*
$$

Note the following facts:

\begin{equation}
Z_t = R_t + \gamma Z_{t+1}
\end{equation}

\begin{equation}
\mathbb{E}\left[(Z_t-s)^-\right] = \mathbb{E}\left[(Z_t-s)\mathbb{1}(Z_t\le s)\right]
\end{equation}

\begin{equation}
\mathbb{E}[H(Z)] = \sum_i p_i \mathbb{E}[H(Z_i)]
\end{equation}

The last equation holds if $Z\sim p_i$ is a probability mixture for any function $H$.
%Here $Z_{t+1} \sim P(x_{t+1}\given x_t, a)$ represents the random variable mixture when we do not yet know which transition was sampled.
%We further simplified the notation by assigning indices $i$ to each of the possible $x_{t+1}$.

We can rewrite the criterion as
\begin{equation}
\begin{split}
\mathbb{E}\left[(Z_t-s)^-\right] &= \mathbb{E}\left[(Z_t-s)\mathbb{1}(Z_t\le s)\right] = \mathbb{E}\left[(R_t + \gamma Z_{t+1}-s)\mathbb{1}(Z_{t+1}\le \dfrac{s - R_t}{\gamma})\right]\\
&= \sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[(r_t + \gamma Z(x_{t+1})-s)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\gamma\left(Z(x_{t+1})-\dfrac{s-r_t}{\gamma}\right)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \gamma\sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\left(Z(x_{t+1})-\dfrac{s-r_t}{\gamma}\right)\mathbb{1}(Z(x_{t+1})\le \dfrac{s - r_t}{\gamma})\right]\\
&= \gamma\sum_{x_{t+1}, r_t} P(x_{t+1}, r_t \given x_t, a)\mathbb{E}\left[\left(Z(x_{t+1}) - \dfrac{s - r_t}{\gamma}\right)^-\right]
\end{split}
\end{equation}


Now let's say we sampled reward $\hat{r}_t$ and state $\hat{x}_{t+1}$, we are still trying to find a policy $\pi^*$ that maximizes 
\begin{equation}\label{eq:sampled x_t+1}
\begin{split}
\pi^* &=\text{arg}\max_\pi \mathbb{E}\left[(Z_t-s)^-\right | \hat{x}_{t+1}, \hat{r}]\\
&= \text{arg}\max_\pi \mathbb{E}\left[\left(Z(\hat{x}_{t+1}) - \dfrac{s - \hat{r}_t}{\gamma}\right)^-\right]
\end{split}
\end{equation}

Where we ignored the unsampled states (since these are not a function of $\hat{x}_{t+1}$) and the multiplicative constant $\gamma$ that will not affect the maximum argument.

At the starting state, we set $s=s^*$. At each following state we select an action according to equation \ref{eq:sampled x_t+1}. By induction we maximize the criterion \ref{eq:var-based criterion} in each step.
\end{proof}

\begin{algorithm}
\caption{VaR-based policy improvement}
\label{alg:var}
\begin{algorithmic}
    \STATE \textbf{input} $\alpha, x_0, \gamma$
    \STATE $a = \text{arg}\max_a CVaR_\alpha(Z(x_0, a))$
    \STATE $s = VaR_\alpha(Z(x_0, a))$
    \STATE $x_t, r_t = \text{envTransition}(x_0, a)$
    \WHILE{$x_t$ is not terminal}
    	\STATE $s = \dfrac{s-r_t}{\gamma}$
    	\STATE $a = \text{arg}\max_a \mathbb{E}\left[(Z(x_t, a)-s)^- \right]$
    	\STATE $x_t, r_t = \text{envTransition}(x_t, a)$
   	\ENDWHILE
\end{algorithmic}
\end{algorithm}

Note that the resulting policy is nonstationary, however we do not need an extended state-space to follow this policy, it is only necessary to remember our previous $s$.

\subsection{Repeated policy improvement}
This policy then could be evaluated again by the distributional Q-learning procedures, however we 

\section{Experiments}
\subsection{Cliffworld}

\subsection{Atari?}

\section{Summary}


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************



\section{$\xi$-computation}


CVaR decomposition formulation (based on the dual):
\begin{equation}\label{eq:cvardecomp}
\begin{split}
CVaR_\alpha(x, a)&=\min_{\xi} \sum_{x'} p(x, a, x')\xi(x') CVaR_{\xi\alpha}(x')\\
\text{s.t.}\quad &\sum_{x'}p(x, a, x')\xi(x')=1\\
&0 \le \xi(x')\le \dfrac{1}{\alpha}
\end{split}
\end{equation}

\begin{theorem}
Solution to minimization problem \ref{eq:cvardecomp} can be computed without optimization by setting
\begin{equation}\label{eq:xi-claim}
\xi(x') = \dfrac{F_{x'}(F^{-1}_x(\alpha))}{\alpha}
\end{equation}
\end{theorem}

\begin{proof}
For simplification, we work only with two states: $x'$ the actual sampled state and $\bar{x}'$ representing the other states. The equation then simplifies to

\begin{equation}\label{eq:cvardecomp2}
\begin{split}
CVaR_\alpha(x, a)&=\min_{\xi} \, p\xi CVaR_{\xi\alpha}(x') + (1-p)\dfrac{1-p\xi}{1-p}CVaR_{\frac{1-p\xi}{1-p}\alpha}(\bar{x}')\\
&=\min_{\xi} \, p\xi CVaR_{\xi\alpha}(x') + (1-p\xi)CVaR_{\frac{1-p\xi}{1-p}\alpha}(\bar{x}')\\
\end{split}
\end{equation}

To find the min we first find the first derivative\footnote{
We used the following facts:
\begin{equation*}
\dfrac{\partial CVaR_{\alpha\xi}}{\partial \xi} = \frac{1}{\xi}VaR_{\xi\alpha}-\frac{1}{\xi}CVaR_{\xi\alpha}\quad\quad\quad
\dfrac{\partial CVaR_{\frac{1-p\xi}{1-p}\alpha}}{\partial\xi} = \frac{p}{1-p\xi}CVaR_{\frac{1-p\xi}{1-p}\alpha}	-	\frac{p}{1-p\xi}VaR_{\frac{1-p\xi}{1-p}\alpha}
\end{equation*}
} w.r.t. $\xi$

\begin{equation}\label{eq:varvar}
\begin{split}
\dfrac{\partial CVaR_\alpha}{\partial \xi} &= pCVaR_{\xi\alpha} + p\xi \dfrac{\partial CVaR_{\alpha\xi}}{\partial \xi} - pCVaR_{\frac{1-p\xi}{1-p}\alpha} + (1 - p\xi)\dfrac{\partial CVaR_{\frac{1-p\xi}{1-p}\alpha}}{\partial\xi}\\
&= pCVaR_{\xi\alpha} + p\xi\left[	\frac{1}{\xi}VaR_{\xi\alpha}-\frac{1}{\xi}CVaR_{\xi\alpha}	\right] - pCVaR_{\frac{1-p\xi}{1-p}\alpha} + (1-p\xi)\left[	\frac{p}{1-p\xi}CVaR_{\frac{1-p\xi}{1-p}\alpha}	-	\frac{p}{1-p\xi}VaR_{\frac{1-p\xi}{1-p}\alpha}\right]\\
&= pCVaR_{\xi\alpha} + pVaR_{\xi\alpha} - pCVaR_{\xi\alpha} - pCVaR_{\xi\alpha} - pCVaR_{\frac{1-p\xi}{1-p}\alpha} + CVaR_{\frac{1-p\xi}{1-p}\alpha} - pVaR_{\frac{1-p\xi}{1-p}\alpha}\\
&= pVaR_{\xi\alpha} - pVaR_{\frac{1-p\xi}{1-p}\alpha}
\end{split}
\end{equation}

By setting the derivative to 0 (to find the min), we get
\begin{equation}\label{eq:varvar}
VaR_{\xi\alpha}(x')= VaR_{\frac{1-p\xi}{1-p}\alpha}(\bar{x}')
\end{equation}

By inserting claim \ref{eq:xi-claim} into \ref{eq:varvar} we get the symmetrical claim
\begin{equation}
\dfrac{1-p\xi}{1-p} = \xi(\bar{x}') = \dfrac{F_{\bar{x}'}(F^{-1}_x(\alpha))}{\alpha}
\end{equation}

We rewrite \ref{eq:cvardecomp2} as (assuming $\xi$ is the minimum point)

\begin{equation}
\begin{split}
\frac{1}{\alpha} \int_0^\alpha F^{-1}_{x}(t)dt &= p\xi \frac{1}{\xi\alpha} \int_0^{\xi\alpha} F^{-1}_{x'}(t)dt + (1-p\xi)\frac{1-p}{(1-p\xi)\alpha} \int_0^{\frac{1-p\xi}{1-p}\alpha} F^{-1}_{\bar{x}'}(t)\\
&=p \frac{1}{\alpha} \int_0^{\xi\alpha} F^{-1}_{x'}(t)dt + (1-p)\frac{1}{\alpha} \int_0^{\frac{1-p\xi}{1-p}\alpha} F^{-1}_{\bar{x}'}(t)
\end{split}
\end{equation}

This must also hold if we multiply both sides by $\alpha$
\begin{equation}
\int_0^\alpha F^{-1}_{x}(t)dt = p\int_0^{\xi\alpha} F^{-1}_{x'}(t)dt + (1-p)\int_0^{\frac{1-p\xi}{1-p}\alpha} F^{-1}_{\bar{x}'}(t)
\end{equation}
And we take derivations w.r.t. $\alpha$ of both sides
\begin{equation}
F^{-1}_{x}(\alpha) = p\xi F^{-1}_{x'}(\xi\alpha) + (1-p\xi) F^{-1}_{\bar{x}'}(\frac{1-p\xi}{1-p}\alpha)
\end{equation}


By inserting \ref{eq:xi-claim} we get
\begin{equation}
\begin{split}
 p\xi F_{x'}^{-1}(\xi\alpha) + (1-p)\xi_2 F_{\bar{x}'}^{-1}\left(\xi_2\alpha\right) &= p\xi F_{x'}^{-1}(F_{x'}(F^{-1}_x(\alpha))) + (1-p\xi) F_{\bar{x}'}^{-1}\left(F_{\bar{x}'}(F^{-1}_x(\alpha))\right)\\
 &= p\xi F_x^{-1}(\alpha) + (1-p\xi)F_x^{-1}(\alpha) = F_x^{-1}(\alpha)
\end{split}
\end{equation}

We've shown that the proposed solution \ref{eq:xi-claim} satisfies the minimization constraint \ref{eq:varvar} (= is a minimal point) and satisfies the dual decomposition \ref{eq:cvardecomp}. (This has been shown only in the differentiated form )

\end{proof}

