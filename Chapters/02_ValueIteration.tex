%************************************************
\chapter{Value Iteration with CVaR}\label{ch:vi}
%************************************************

Value iteration is a standard algorithm for maximizing expected discounted reward used in reinforcement learning. In this chapter we extend the results of \citet{chow2015risk}, who have recently proposed an approximate value iteration algorithm for CVaR MDPs. 

The original algorithm requires the computation of a linear program in each step of the value iteration procedure. Utilizing a connection between the used $\alpha \cvar_\alpha$ function and the quantile function, we sidestep the need for this computation and propose a linear-time version of the algorithm, making CVaR value iteration feasible for much larger MDPs. 

After reminding the reader of the standard value iteration algorithm, we present the original algorithm in \secref{vi:cvar}. The improved algorithm is presented in  \secref{vi:linear}, followed by section \secref{vi:experiments}, where we test the algorithm on selected environments.

%*****************************************

\subsection{Value Iteration}

Value iteration \citep{sutton1998reinforcement} is a well-known algorithm for computing the optimal (action-)value function and hereby finding the optimal policy. Let us remind ourselves of the Bellman optimality operator $\cT$ \eqnref{bellmanoptimalityoperator}:
\begin{equation*}
\cT Q(x,a) := \expval{ R(x, a)} + \gamma \expect\nolimits_{P} \bsquare{\max_{a' \in \cA}Q(x', a')}
\end{equation*}
or rewritten for the value function $V$
\begin{equation}
\cT V(x) = \max_a\braces{r(x, a) + \gamma\sum_{s'}p(s'|x, a)V(x')}
\end{equation}
As stated before, $\cT$ is a contraction (\secref{contraction}). This means that by repeatedly applying the operator we eventually converge to the optimal point, since we converge and the definition holds in this point. This leads to the formulation of the \textit{Value Iteration} algorithm. The only difference between theory and practice is the introduction of a small parameter $\epsilon$ that allows us to check the converge and end the algorithm when we reach a certain precision, as the contraction converges only in the limit.

See \algref{vi}.


\begin{algorithm}
\caption{Value Iteration}
\label{alg:vi}
\begin{algorithmic}
    \STATE Initialize $V$ arbitrarily (e.g. $V(x)=0$ for all $x \in \cX$)
    
	\REPEAT
	
	\STATE $v = V(x)$
	\STATE $\Delta = 0$
	
	\FOR{each $x \in \cX$}
	\STATE $V(x) = \max_a\braces{r(x, a) + \gamma\sum_{s'}p(s'|x, a)V(x')}$
	\STATE $\Delta = \max\braces{\Delta, |v-V(x)|}$
	\ENDFOR
	
	\UNTIL{ $\Delta < \epsilon$ }
	
	\STATE Output a deterministic policy $\pi \approx \pi^*$:
   	\STATE $\quad\quad \pi(x) = \argmax_a\braces{r(x, a) + \gamma\sum_{s'}p(s'|x, a)V(x')}$
\end{algorithmic}
\end{algorithm}


\section{CVaR Value Iteration}\label{sec:vi:cvar}

\citet{chow2015risk} present a dynamic programming formulation for the CVaR MDP problem \eqnref{problem}. As CVaR is a time-inconsistent measure, their method requires an extension of the state space. A Value Iteration type algorithm is then applied on this extended space and \citet{chow2015risk} proved it's convergence. 

We repeat their key ideas and results bellow, as they form a basis for our contributions presented in later sections. The results are presented with our notation introduced in \chref{prelim}, which differs slightly from the paper, but the core ideas remain the same.

\subsection{Bellman Equation for CVaR}

The results of \citet{chow2015risk} heavily rely on the CVaR decomposition theorem (Lemma 22, \citep{pflug2016time}):
%
\begin{equation}\label{eqn:cvardecomp}
\cvar_\alpha\bround{Z^\pi(x)} = \min_{\xi \in \envelope} \sum_{x'} p(x'| x, \pi(x))\xi(x') \cvar_{\xi(x')\alpha}\bround{Z^\pi(x')}
\end{equation}
%
where the risk envelope $\envelope$ coincides with the dual definition of CVaR \eqnref{envelope}.
The theorem states that we can compute the $\cvar_\alpha\bround{Z^\pi(x, a)}$ as the minimal (or worst-case) weighted combination of $\cvar_\alpha\bround{Z^\pi(x')}$ under a probability distribution perturbed by $\xi(x')$.

Note that the decomposition requires only the representation of CVaR at different confidence levels and not the whole distribution at each level, which we might be tempted to think because of the time-inconsistency issue.

\citet{chow2015risk} extend the decomposition theorem by defining the \emph{CVaR value function}\footnote{We use $C$ instead of $V$ in our notation.} $C(x, y)$ with an augmented state-space $\mathcal{X}\times\mathcal{Y}$ where $\mathcal{Y}=(0,1]$ is an additional continuous state that represents the different confidence levels.
%
\begin{equation}\label{eqn:cdef}
C(x, y)=\max_{\pi \in \Pi_H} \cvar_{y}\bround{Z^\pi(x)}
\end{equation}
%
Similar to standard dynamic programming, it is convenient to work with with operators defined on the space of value functions. This leads to the following definition of the CVaR Bellman operator $\mathbf{T}:\mathcal{X}\times\mathcal{Y}\to\mathcal{X}\times\mathcal{Y}$:
%
\begin{equation}
\mathbf{T}C(x, y) = \max_a \bsquare{ R(x, a) + \gamma \min_{\xi \in \envelope} \sum_{x'} p(x'| x, a)\xi(x') C\bround{x', y\xi(x')}}
\end{equation}
%
or in our simplified notation, this describes the following relationship:
%
\begin{equation}\label{eqn:tcvar}
\mathbf{T} \cvar_y(Z(x))=\max_a \bsquare{R(x, a) + \gamma \cvar_{y}(Z(x, a))}
\end{equation}
%
\citet{chow2015risk} further showed (Lemma 3) that the operator $\mathbf{T}$ is a contraction and also preserves the convexity of $y \cvar_t$. The optimization problem \eqnref{cvardecomp} is a convex one and therefore has a unique solution. Additionally, the fixed point of this contraction is the optimal $C^*(x, y) = \max_{\pi \in \Pi} \cvar_y (Z^\pi(x, y))$ (\citep{chow2015risk}, Theorem 4).
 
Naive value iteration with operator $\mathbf{T}$ is unfortunately unusable in practice, as the state space is continuous in $y$. The solution proposed in \cite{chow2015risk} is then to represent the convex $y\cvar_y$ as a piecewise linear function. 

\subsection{Value Iteration with Linear Interpolation}

Given a set of $N(x)$ interpolation points $\mathbf{Y}(x) = \braces{y_1, \dots, y_{N(x)}}$, we can interpolate the $yC(x,y)$ function on these points, i.e.
%
\begin{equation*}
\interpI_{x}[C](y)=y_iC(x,y_{i})+\frac{y_{i+1}C(x,y_{i+1})-y_iC(x,y_{i})}{y_{i+1}-y_i}(y-y_i),
\end{equation*}
%
where $y_i = \max \left\{y'\in \mathbf{Y}(x) : y' \leq y\right\}$
The interpolated Bellman operator $\mathbf{T}_\interpI$ is then also a contraction and has a bounded error (\citep{chow2015risk}, Theorem 7). 
\todo{bounded -> linear in $\theta$}
%
\begin{equation}\label{eqn:linearbellman}
\mathbf{T}_\interpI C(x, y) = \max_a \bsquare{ R(x, a) + \gamma \min_{\xi \in \envelope} \sum_{x'} p(x'| x, a)\dfrac{\interpI_{x'} [C](y\xi(x'))}{y}}
\end{equation}
%
The full value iteration procedure is presented in \algref{cvarlinear}. 

This algorithm can be used to find an approximate global optimum in any MDP. There is however the issue of computational complexity. As the algorithm stands, the straightforward approach is to solve each iteration of \eqnref{linearbellman} as a linear program, since the problem is convex and piecewise linear, but this is not practical, as the LP computation can be demanding and is therefore not suitable for large state-spaces.

\unclear{maybe formulate the LP exactly?}

\begin{algorithm}[h]
\caption{CVaR Value Iteration with Linear Interpolation (Algorithm 1 in \citep{chow2015risk})}
\label{alg:cvarlinear}
1: \textbf{Given:}
\begin{itemize}
%\item An interpolation error bound $\epsilon>0$ for small CVaR thresholds.
\item $N(x)$ interpolation points $\mathbf{Y}(x)  = \left\{y_1,\dots,y_{N(x)}\right\} \in [0,1]^{N(x)}$ for every $x\in \mathcal X$ with $y_i<y_{i+1}$, $y_1=0$ and $y_{N(x)}=1$.
%, $y_2 = \min_{x',a}\{ P(x'|x,a):P(x'|x,a)\neq 0\}$
%\item An interpolation function $\interpI_{x}[V](y;\interpY(x))$ for $yV(x,y)$ for any arbitrary value function $V$.
\item Initial value function $C_0(x,y)$ that satisfies:
\begin{enumerate}
\item $yC_0(x,y)$ is convex in $y$ for all $x$
\item $yC_0(x,y)$ is continuous in $y$ for all $x$
\end{enumerate}
 %where $C_0(x,y)=0$ at $y<0$.
\end{itemize}
2: Repeat until convergence:
\begin{itemize}
%\item Update the smallest non-zero grid $y_2$ in $\interpY(x)$ by choosing it to satisfy $\max_{x\in\mathcal X, y\in \mathbf I_2(x)}|C_0(x,y_2)-C_0(x,y)|\leq\epsilon$, where the interpolation based Bellman operator $ \bellint$ is given by
%  \[
%\hspace{-0.5in}  \bellint[C](x,y) =
% \min_{a\in\mathcal A}\left[C(x,a)+\gamma\max_{\xi\in \U_{\text{CCaR}}(y, P(\cdot|x,a))}\sum_{x'\in\mathcal X}\frac{\interpI_{x'}[C](y\xi(x');\interpY(x'))}{y}P(x'|x,a)\right].
%   \]
\item For each $x \in \mathcal X$ and each $y_i\in \mathbf{Y}(x)$, update the value function estimate as follows:
  \begin{equation*}
   C_{k+1}(x,y_i)= \mathbf{T}_\interpI[C_k](x,y_i),
  \end{equation*}
  \end{itemize}
3: Set the converged value iteration estimate as $\widehat{C}^*(x,y_i)$, for any $x\in\mathcal X$, and $ y_i\in\mathbf{Y}(x)$.
\end{algorithm}

\subsection{Optimal policy}
An important product of any value iteration algorithm is the optimal policy. The value-function $C^*$ can be used to extract the optimal policy $\pi^*$ of the original problem \eqnref{problem}, using the following theorem.

\begin{theorem}[Optimal Policies, Theorem 5 in \citep{chow2015risk}]\label{thm:optimalpolicy}
Let $\pi_H^*=\{\mu_0,\mu_1,\ldots\}\in\Pi_H$ be a history-dependent policy recursively defined as:
\begin{equation}\label{eqn:policy_construct}
\mu_k(h_k) = u^*(x_k, y_k),\,\,\forall k\geq 0,
\end{equation}
with initial conditions $x_0$ and $y_0=\alpha$, and state transitions
\begin{equation}\label{eqn:opt_state}
x_k\sim P(\cdot\mid x_{k-1},u^*(x_{k-1},y_{k-1})),\quad y_k = y_{k-1}\xi_{x_{k-1},y_{k-1},u^*}^*(x_k), \forall k\geq 1,
\end{equation}
where the stationary Markovian policy $u^*(x,y)$ and risk factor $\xi_{x,y,u^*}^*(\cdot)$ are solution to the  max-min optimization problem in the CVaR Bellman operator $\mathbf T[C^*](x,y)$.
Then, $\pi^*_H$ is an optimal policy for problem \eqnref{problem} with initial state $x_0$ and CVaR confidence level $\alpha$.
\end{theorem}

And this holds for both the original operator $\mathbf{T}$ and the linearly interpolated $\mathbf{T}_\interpI$.

%*****************************************

\section{Efficient computation using quantile representation}\label{sec:vi:linear}

We present our original contributions in this section. We first describe a connection between the $y\cvar_y$ function and the quantile function of the underlying distribution. We then use this connection to formulate a faster computation of the value iteration step, resulting in the first linear-time algorithm for solving CVaR MDPs with bounded error.

\begin{lemma}\label{thm:varcvarconnection}
Any discrete distribution has a piecewise linear and convex $y\cvar_y$ function. Similarly, any piecewise linear convex function can be seen as representing a certain discrete distribution.
\\
Particularly, the integral of the quantile function is the $y\cvar_y$ function
\begin{equation}\label{eqn:varcvarintegration}
y\cvar_y(Z) = \int_0^y \var_\beta(Z) \dt \beta
\end{equation}
and the derivative of the $y\cvar_y$ function is the quantile function
\begin{equation}\label{eqn:varcvarderivation}
\dfrac{\partial}{\partial y} y \cvar_y(Z) = \var_y(Z)
\end{equation}
\end{lemma}

\begin{proof}
The fact that discrete distributions have a piecewise linear $\ycvary$ function has already been shown by \citet{rockafellar2000optimization}.
\\
According to definition \eqnref{cvardef} we have
\begin{equation*}
y\cvar_y(Z) = y\dfrac{1}{y}\int_0^y \var_\beta(Z) \dt \beta = \int_0^y \var_\beta(Z) \dt \beta
\end{equation*}
by taking the $y$ derivative, we have
\begin{equation*}
\dfrac{\partial}{\partial y} y \cvar_y(Z) = \dfrac{\partial}{\partial y} \int_0^y \var_\beta(Z) d\beta = \var_y(Z)
\end{equation*}
\end{proof}

You can get some intuition from \figref{cvarvisual}, where the integral-derivation relationship is clearly visible.

According to Lemma \ref{thm:varcvarconnection}, we can reconstruct the $y\cvar_y$ from the underlying distribution and vice-versa. We utilize the fact that the conversion is linear in the number of probability atoms to formulate a fast way of computing the $\mathbf{T}_\interpI$ operator.

\begin{figure}
\center
\includegraphics[width=\linewidth]{gfx/cvar_visualized.pdf}
\caption{Comparison of a discrete distribution and it's approximation according to the CVaR linear interpolation operator.}
\label{fig:cvarvisual}
\end{figure}

\subsection{CVaR Computation via Quantile Representation}

We propose the following procedure: instead of using linear programming for the CVaR computation, we use the underlying distributions represented by the $\alpha \cvar_\alpha$ function to compute CVaR at each atom. The general steps of the computation are as follows

%\begin{algorithm}[h]
%\caption{Value Iteration via Quantile Representation}
%\label{alg:fastvi}
%\begin{enumerate}
%\item transform $y \cvar_y$ of each possible state transition to a discrete probability distribution function using \eqnref{varcvarderivation}
%\item combine these to to a distribution representing the full state-action distribution
%\item compute $y \cvar_y$ for all atoms using \eqnref{varcvarintegration}
%\end{enumerate}
%\end{algorithm}

\begin{enumerate}
\item transform $y \cvar_y$ of each possible state transition to a discrete probability distribution using \eqnref{varcvarderivation}
\item combine these to to a distribution representing the full state-action distribution
\item compute $y \cvar_y$ for all atoms using \eqnref{varcvarintegration}
\end{enumerate}
See \figref{cvarcomputation} for a visualization of the procedure. 
\\
Note that this procedure is linear for discrete distributions. For completeness, we show the explicit computation in **appendix**. \todo{appendix}

\begin{figure}
\center
\includegraphics[width=\linewidth]{gfx/cvar_vi_conversion.pdf}
\caption{Visualization of the CVaR computation for a single state and action with two transition states. Thick arrows represent the conversion between $\ycvary$ and the quantile function.}
\label{fig:cvarcomputation}
\end{figure}

To show the correctness of this approach, we formulate it as a solution to the problem \eqnref{cvardecomp}. Note that we skip the reward and gamma scaling for readability's sake. The extension to the bellman operator is trivial.

\subsection{$\xi$-computation}

Similarly to \thmref{optimalpolicy}, we need a way to compute the $y_{k+1}=y_{k}\xi^*(x_k)$ to extract the optimal policy. We compute $\xi^*(x_k)$ by using the following intuition: $y_{k+1}$ is the portion of $Z(x_{k+1})$ that is present in $\cvar_{y_k}(Z(x_k))$. In the continuous case, it is the probability in $Z(x_{k+1})$ before the $\var_{y_k}(Z(x_k))$ as we show bellow.

\begin{theorem}
Let $x_1', x_2'$ be two states reachable from state $x$ in a single transition. Let the cumulative distribution functions of the state's underlying distributions $Z(x_1'), Z(x_2)$ be strictly increasing with unbounded support.
Then the solution to minimization problem \eqnref{cvardecomp} can be computed setting
\begin{equation}\label{eqn:xi-claim}
\xi ( x_i' ) = \dfrac{F_{x_i'}(F^{-1}_x(\alpha))}{\alpha} 
\end{equation}
\end{theorem}

\begin{proof}
Since we are interested in the minimal argument, we can ease the computation by focusing on the $\acvara$ function instead of $\cvar_\alpha$.
When working with two states, the equation of interest simplifies to

\begin{equation}\label{eqn:cvardecomp2}
\begin{split}
\acvara(Z(x))&=\min_{\xi} \, p_1\xi_1\alpha \cvar_{\xi_1\alpha}(Z(x_1')) + (\xi_2\alpha\cvar_{\xi_2\alpha}(Z(x_2'))\\
\text{s.t.}& \quad p_1\xi_1 + p_2\xi_2 = 1\\
& 0 \le \xi_1 \le \frac{1}{\alpha}
\end{split}
\end{equation}

\begin{equation}\label{eqn:cvardecomp3}
\begin{split}
\acvara(Z(x))&=\min_{\xi} \, p_1\xi_1\alpha \cvar_{\xi_1\alpha}(Z(x_1')) + (1-p_1)\dfrac{1-p_1\xi_1}{1-p_1}\alpha\cvar_{\frac{1-p_1\xi_1}{1-p_1}\alpha}(Z(x_2'))\\
&=\min_{\xi} \, p_1\int_0^{\xi_1\alpha}\var_{\beta}(Z(x_1'))\,\text{d}\beta + (1-p_1)\int_0^{\frac{1-p_1\xi_1}{1-p_1}\alpha}\var_{\beta}(Z(x_2')\,\text{d}\beta\\
\end{split}
\end{equation}

To find the minimal argument, we find the first derivative w.r.t. $\xi_1$

\begin{equation}
\begin{split}
\dfrac{\partial \acvara}{\partial \xi_1} &= p_1\alpha \var_{\xi_1\alpha}(Z(x_1')) + (1-p_1)\alpha\dfrac{-p_1}{1-p_1}\var_{\frac{1-p_1\xi_1}{1-p_1}\alpha}(Z(x_2')\\
&= p_1\var_{\xi\alpha}(Z(x_1')) - p_1\var_{\frac{1-p\xi}{1-p}\alpha}(Z(x_2'))
\end{split}
\end{equation}

By setting the derivative to 0 , we get
\begin{equation}
\var_{\xi_1\alpha}(Z(x_1')) \overset{!}{=} \var_{\frac{1-p\xi}{1-p}\alpha}(Z(x_2')) = \var_{\xi_2\alpha}(Z(x_2'))
\end{equation}

\citet{bernard2015quantile} have shown that in the case of strictly increasing c.d.f. with unbounded support, it holds that
\begin{equation}
\begin{split}
\var_{\xi_1\alpha}(Z(x_1')) &= \var_{\xi_2\alpha}(Z(x_2')) &&= \var_\alpha(Z(x))\\
F^{-1}_{Z(x_1')}(\xi_1\alpha) &= F^{-1}_{Z(x_2')}(\xi_2\alpha) &&= F^{-1}_{Z(x)}(\alpha)
\end{split}
\end{equation}
and we can extract the values of $\xi_1\alpha, \xi_2\alpha$ using the 
\begin{equation}
\begin{split}
&F^{-1}_{Z(x_1')}(\xi_1\alpha) &&= F^{-1}_{Z(x)}(\alpha) &&&/ \, F_{Z(x_1')}\\
&F_{Z(x_1')}\bround{F^{-1}_{Z(x_1')}(\xi_1\alpha)} &&= F_{Z(x_1')}\bround{F^{-1}_{Z(x)}(\alpha)}\\
&\xi_1\alpha &&= F_{Z(x_1')}\bround{F^{-1}_{Z(x)}(\alpha)}
\end{split}
\end{equation}

And similarly for $\xi_2$.

Since the problem is convex, we have found the optimal point.

\end{proof}

\todo{explain why the general proof may be difficult - }

The theorem is ***easily*** extended to multiple states by induction. We conjecture that similar claim holds for general distributions, however this would require more technical arguments and is out of scope of this thesis. See \citet{bernard2015quantile} for details on the two-dimensional case for general distributions.


%*****************************************



\section{Experiments}\label{sec:vi:experiments}

We test the proposed algorithm on the same task as \citet{chow2015risk}. The task of the agent is to navigate on a rectangular grid to a given destination, moving in it's four-neighborhood. To encourage fast movement towards the goal, the agent is penalized for each step by receiving a reward -1. A set of obstacles is placed randomly on the grid and stepping on an obstacle ends the episode while the agent receives a reward of -40.
To simulate sensing and control noise, the agent has a $\delta=0.05$ probability of moving to a different state than intended.

For our experiments, we choose a $40 \times 60$ grid-world. We approximate the $\acvara$ function using 21 log-spaced atoms with $\theta=2$ as in \citep{chow2015risk}. The learned policies on a sample grid are shown in \figref{vigrid}.

While \citet{chow2015risk} report computation time on the order of two hours, our naive Python implementation converges in a matter of ***minutes*** \todo{exact time here pls}.

\begin{figure}[h]
\center
\includegraphics[width=\linewidth]{gfx/vi_optimal_paths.pdf}
\caption{Grid-world simulations. The optimal deterministic paths are shown together with value functions for given $\alpha$.}
\label{fig:vigrid}
\end{figure}

\todo{section on policy problems}

\section{Summary}

\todo{discuss similarity with the distributional VI}
The proposed method draws remarkable similarity to distributional RL (\secref{distributional}). In fact, the one-step update is identical before picking the actions. The main differences between the two approaches are 1) action selection - in the distributional approach, we select a single action (based on the expected value) for the whole distribution contrary to CVaR VI, where we select a different action for each $\alpha$-level; 2) the approximation step - in distributional RL, we try to minimize the Wasserstein distance of the selected distributions. In CVaR VI, we construct the distribution, so that it's CVaR at given confidence levels is identical to the exact distribution.

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************



